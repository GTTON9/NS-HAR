---
title: "KF_Residual"
output: html_document
date: "2025-04-15"
---

```{r setup, include=FALSE}
BF_data
BF_5year <- BF_data[2518:3847]
BF_5year
```

```{r}
library(MASS)
library(ggplot2)
library(tibble)
library(rugarch)
```

# Functions


```{r}
basis <- function(x, degree, i, knots){
  if(degree == 0){
    B <- ifelse((x>=knots[i])&(x<knots[i+1]),1,0)
  } else {
    if((knots[degree + i] - knots[i]) == 0){
      if(x != knots[i+degree]){
        alpha1 <- 0
      } else {
        return(1)
      }
    } else {
      alpha1 <- (x-knots[i])/(knots[degree+i] - knots[i])
    }
    if((knots[i+degree+1] - knots[i+1]) == 0){
      if(x != knots[i+degree]){
        alpha2 <- 0
      } else {
        return(1)
      }
    } else {
      alpha2 <- (knots[i+degree+1] - x) / (knots[i+degree+1] - knots[i+1])
    }
    B <- alpha1 * basis(x, (degree-1), i, knots) + 
      alpha2*basis(x, (degree-1), (i+1), knots)
  }
  return(B)
}
```

Construct the matrix $\mathbf{B}$, column by column. Matrix $B$ contains the basis functions computed through the Cox-deBoor recursive formulas using the function defined in the codechunk above, evaluated at different times to maturity (rows) and at different knot intervals (columns). For more details, check FURM on canvas.

```{r}
matrix_b <- function(x, degree=3, int_knots) { 
  # the x argument takes in a vector of time values that 
  # will be used to evaluate a design matrix of basis functions 
  # the degree argument specifies the highest degree of polynomials for
  # the basis functions
  # the int_knots argument takes in a vector of knots that will be used 
  # to determine the intervals of the piecewise function
  bound_knots <- int_knots[c(1, length(int_knots))] # this line creates bound knots
  knots <- c(rep(bound_knots[1], (degree+1)), int_knots[c(-1, -length(int_knots))], rep(bound_knots[2], (degree+1)))
  # the line above adds a couple of extra knots to each end of the int_knots vector because of the Cox-deBoor recursion
  K <- length(int_knots) + degree - 1 # number of columns in the Basis matrix
  B.mat <- matrix(0,nrow = length(x), ncol = K) # initialize the matrix
  for(j in 1:K) {
    B.mat[,j] <- sapply(X = x, FUN = basis, degree = degree, i = j, knots = knots) # add each column, one by one
  }
  return(B.mat) # return the matrix
}
```


```{r}
interp_yc <- function(yield_list, int_knots, degree = 3, d, last_tenor){
  # yield_list: parameter of the form of a list of data frames containing ZCB spot rate
  # int knots: the interior knots used for b-spline construction
  # degree: highest degree of polynomials for the basis functions
  # d: the date chosen to interpolate from the list
  # last_tenor: last tenor to interpolate in a day
  yield_list[[d]] <- data.frame(Maturity = yield_list[[d]]$Maturity,
                                ZERO_YLD1 = yield_list[[d]]$ZERO_YLD1)
  yc_df_pre <- rbind(data.frame(Maturity = 0, ZERO_YLD1 = 0), na.omit(yield_list[[d]]))
  last_row <- which(round(yc_df_pre$Maturity,3) == last_tenor)
  yc_df <- yc_df_pre[1:last_row,]
  yields <- c(0, yc_df$ZERO_YLD1)
  maturities <- c(0, as.numeric(yc_df$Maturity))
  x <- as.numeric(maturities) # maturity dates
  B <- matrix_b(x, degree=degree, int_knots = int_knots) 
  B_t_B <- t(B) %*% B
  # B is the design matrix on which the least squares coefficients will be calculated
  
  alphas <- solve(B_t_B) %*% t(B) %*% yields # OLS Formula for coefficients
  x2 <- seq(1/12, last_tenor, 1/12) # this range is used to simulate a continuous yield curve
  B2 <- matrix_b(x2, degree = degree, int_knots = int_knots) 
  # B2 is the matrix of basis functions but evaluated at a 'continuous' time (not really but close enough)
  
  interpolated_yields <- data.frame(Maturity = x2, ZERO_YLD1 = B2 %*% alphas) # create dataframes for plotting
  og_yields <- data.frame(ttm = maturities, yield = yields)

  return(interpolated_yields)
}
```

Interpolate a list of data with different dates.

```{r}
interpolate_list <- function(yield_list, start, T_, degree = 3){
  # yield_list: Parameter of the form of a list of data frames containing ZCB spot rate
  # start: starting date from the yield_list list
  # T_: length of time window
  # degree: highest degree of polynomials for the basis functions
  interpolated_yc <- list()
  k <- 1
  for(i in start:(start + T_ - 1)){
    lt_max <- max(yield_list[[i]]$Maturity) # This line of code basically chops all yields beyond 20
    avail_ylds <- na.omit(yield_list[[i]]$ZERO_YLD1)
    maturities <- yield_list[[i]]$Maturity
    N <- length(avail_ylds)
    if(N %in% c(5, 6)){
      int_knots <- c(0, quantile(maturities, probs = c(0, 0.5, 1)))
    } else if(N %in% c(7,8,9)){
      int_knots <- c(0,quantile(maturities, probs = c(0, 0.33, 0.66, 1)))
    } else if(N %in% 10:15){
      int_knots <- c(0,quantile(maturities, probs = c(0, 0.25, 0.5, .75, 1)))
    } else {
      int_knots <- c(0,quantile(maturities, probs = c(0, 0.20, 0.4, .6, .8, 1)))
    }
    interpolated_yc[[k]] <- interp_yc(yield_list = yield_list,
                                      int_knots = int_knots,
                                      d = i,
                                      last_tenor = lt_max,
                                      degree = degree)[3:240,]
    k <- k + 1
  }
  return(interpolated_yc)
}
```

Kalman Filter basis calculation 
```{r}
NS_loading <- function(lambda, mats) {
  loading <- rbind(rep(1, length(mats)),
                   (1 - exp(-lambda*mats)) / (lambda*mats),
                   (1 - exp(-lambda*mats)) / (lambda*mats) - exp(-lambda*mats))
  return(loading)
}
```


Standard Kalman Filter
```{r}
KF_NS <- function(par, yt, mats, lam) {
  # Standard Kalman Filter for Nelson Siegel model
  # Model:
  #   y_t = d_t + F_t' x_t + f_t + v_t, v_t ~ N(0, V), observation equation
  #   x_t = c + G x_{t-1} + w_t, w_t ~ N(0, W), state equation
  #   f_t = b*t + beta*cos(2*pi*t*dt) + eta*sin(2*pi*t*dt), seasonal effect
  # Inputs:
  #   par: a vector of parameters
  #   yt: the logarihm of futures prices
  #   T: maturities
  #   delivery_time: a vector of date, which is necessary if seasonality is "Constant"
  #   dt: delta t
  #   smoothing: a boolean variable indicate if Kalman Smoothing is required
  #   seasonality: "Constant" or "None"
  # Outputs:
  #   nll: the negative log likelihood
  #   ll_table: a vector to store cumulative log-likelihood at each time point - used to calculate Sandwich variance
  #   table_at_filter: a nT*2 matrix gives the filtered values of state variables.
  #   table_at_prediction: a (nT+1)*2 matrix gives the predicted values of state variables.
  #   table_at_smoother: a nT*2 matrix gives the smoothed values of state variables. The algorithm of Kalman Smoother is given by Bierman (1973) and De Jong (1989).
  #   ft: seasonal effect

 # require(lubridate)

  n_obs <- dim(yt)[1]
  n_contract <- dim(yt)[2]

  table_xt_filter <- matrix(0, nrow = n_obs, ncol = 3) # a_t|t
  table_Pt_filter <- array(0, dim = c(3, 3, n_obs)) # P_t|t
  table_xt_prediction <- matrix(0, nrow = n_obs+1, ncol = 3) # a_t|t-1
  table_Pt_prediction <- array(0, dim = c(3, 3, n_obs+1)) # P_t|t-1
  
  nll <- 0 # negative log-likelihood
  ll_table <- matrix(0, nrow = 1, ncol = n_obs) # table of log-likelihood

  table_et <- matrix(0, nrow = n_obs, ncol = n_contract) # e_t
  table_L <- array(0, dim = c(n_contract, n_contract, n_obs)) # Covariance of y
  table_y <- matrix(0, nrow = n_obs, ncol = n_contract) # y_hat

  # Parameters
  if (length(par) != 9+n_contract) {
    stop("Incorrect number of parameters. ")
  }

  phi0 <- c(par[1], par[2], par[3])
  phi1 <- diag(c(par[4], par[5], par[6]))
  x0 <- phi0 / (1 - diag(phi1))
  #lam <- par[7]
  sigma_w <- diag( par[7:9] )
  sigma_v <- diag( par[10: length(par)] )
  
  # Initialization
  xt_filter <- x0
  Pt_filter <- diag( diag(sigma_w) / (1 - diag(phi1)^2) )

  # Kalman Filter
  for (i in 1:n_obs) {
    lambda <- rbind(rep(1, n_contract),
                    (1 - exp(-lam * mats[i,])) / (lam * mats[i,]),
                    (1 - exp(-lam * mats[i,])) / (lam * mats[i,]) - exp(-lam * mats[i,]))

    # Prediction step
    xt_prediction  <- phi0 + phi1 %*% xt_filter # a_t+1|t
    Pt_prediction <- phi1 %*% Pt_filter %*% t(phi1) + sigma_w # P_t+1|t
    y_prediction <- t(lambda) %*% xt_prediction # ytilde_t|t-1 = d_t + F_t a_t|t-1

    # Filter step
    et <- yt[i, ] - t(y_prediction) # e_t = y_t - ytilde_t|t-1
    L <- t(lambda) %*% Pt_prediction %*% lambda + sigma_v # Covariance matrix of et
    invL <- solve(L) # inverse of L
    K <- Pt_prediction %*% lambda %*% invL # Kalman gain matrix: K_t

    xt_filter <- xt_prediction + K %*% t(et) # a_t
    Pt_filter <- (diag(3) - K %*% t(lambda)) %*% Pt_prediction # P_t
    #Pt_filter <- (diag(3) - K %*% t(lambda)) %*% Pt_prediction %*% t(diag(3) - K %*% t(lambda)) + K %*% sigma_v %*% t(K)

    # Update tables
    table_xt_filter[i, ] <- t(xt_filter)
    table_Pt_filter[, , i] <- Pt_filter
    table_xt_prediction[i+1, ] <- t(xt_prediction)
    table_Pt_prediction[, , i+1] <- Pt_prediction
    table_et[i, ] <- et
    table_L[, , i] <- L
    table_y[i, ] <- y_prediction

    if (det(L)<0) {
      message(i)
      message("matrix is not semi positive definite (KF)")
    }

    # Update likelihood
    nll <- nll + 0.5*log(det(L)) + 0.5*et %*% solve(L) %*% t(et)
    ll_table[i] <- -(0.5*length(yt[i, ])*log(2*pi) + 0.5*log(det(L)) + 0.5*et %*% solve(L) %*% t(et))
  }

  return(list(nll = nll,
              ll_table = ll_table,
              xt_filter = table_xt_filter,
              Pt_filter = table_Pt_filter,
              xt_prediction = table_xt_prediction,
              cov_y = table_L,
              y_hat = table_y))
}


```

```{r}
yule_walker_ar1 <- function(ts) {
  # Estimates the parameters of an AR(1) process using the Yule-Walker equations.
  #
  # Parameters:
  # ts: A univariate time series (numeric vector).
  #
  # Returns:
  # A list containing:
  #   phi: The estimated autoregressive coefficient.
  #   sigma2: The estimated variance of the innovations.
  #   c: The estimated intercept term.

  # Compute the sample mean (used for centering the time series)
  mu <- mean(ts)
  
  # Center the time series
  ts_centered <- ts - mu
  
  # Compute the autocovariance at lag 0 and lag 1
  gamma0 <- var(ts_centered) * (length(ts_centered) - 1) / length(ts_centered)
  gamma1 <- sum(ts_centered[-1] * ts_centered[-length(ts_centered)]) / length(ts_centered)
  
  # Estimate phi using the Yule-Walker equation
  phi <- gamma1 / gamma0
  
  # Estimate the intercept c
  c <- mu * (1 - phi)
  
  # Estimate the innovation variance sigma2
  sigma2 <- gamma0 * (1 - phi^2)
  
  # Return the estimates
  return(list(phi = phi, sigma2 = sigma2, c = c))
}

```

OLS for Nelson Siegel
```{r}
fit_nelson_siegel <- function(yield_list, lambda, start, tenors, T_){


  maturities <- tenors
  N <- length(maturities)
  
  term1 <- 1
  term2 <- (1 - exp(-maturities*lambda)) / (maturities*lambda)
  term3 <- ((1 - exp(-maturities*lambda)) / (maturities*lambda)) - exp(-maturities*lambda)
  Phi <- cbind(term1, term2, term3) # Construct Phi matrix for NS
  
  Y_mat <- matrix(0, nrow = N, # matrix of N by T, containing yields for each tenor (columns)
                  ncol = T_) # where each column represents a different date
  j <- 1
  for(t in start:(start + T_- 1)){
    Y_mat[,j] <- yield_list[t]
    j <- j + 1
    phitphi_1phit <- solve(t(Phi) %*% Phi, t(Phi)) # OLS for the coefficients for every single day
    betas_t <- phitphi_1phit %*% Y_mat  
    betas <- rowSums(betas_t) / T_ # average all coefficients
    eps <- matrix(0, nrow = N, ncol = T_) # matrix of errors for each day (column) and each tenor (row)
    for(t in 1:T_){
      eps[,t] <- Y_mat[,t] - Phi %*% betas # Populate errors
    }
    sig_hat2 <- sum(as.vector(eps)^2) / (N * T_ - 3) # take mean squared error (MLE Estimator)
  }

  return(list(betas = betas, # fitted betas static: 1*3   dynamic: T*3
              sigma2 = sig_hat2, # MSE
              lambda = lambda, # lambda(input)
              cov_mat_betas = sig_hat2 * solve(t(Phi) %*% Phi), # Nelson Siegel design matrix N*3
              eps = eps,
              Phi = Phi)) # residuals N*T
  
}
```

```{r}
tenors <- c(1, 5, 10, 30)
dim(BF_5year)
```

```{r}
tsBetas <- matrix(NA, ncol = nrow(BF_5year), nrow = 3)
obs_res <- matrix(NA, ncol = nrow(BF_5year), nrow = 1)
for(i in 1:nrow(BF_5year)) {
  ols_fit <- fit_nelson_siegel(BF_5year, 0.1, tenors = c(5, 10, 20, 30), start = i, T_ = 1)
  tsBetas[,i] <- ols_fit$betas
  obs_res[,i] <- ols_fit$sigma2
}
tsBetas[,1]
obs_res[,]
```

```{r}
AR1_level <- yule_walker_ar1(tsBetas[1,])
AR1_slope <- yule_walker_ar1(tsBetas[2,])
AR1_curv <- yule_walker_ar1(tsBetas[3,])

AR1_level
AR1_slope
AR1_curv
```

```{r}
sigOLS2 <- mean(obs_res) # R: covariance matrix of observation noise (assumed to be diagonal)
tenors <- c(5, 10, 20, 30) # tenor of each time
mats_matrix_real <- matrix(rep(tenors, each = nrow(BF_5year)), nrow = nrow(BF_5year), byrow = FALSE)


parameters <- c(AR1_level$c, AR1_slope$c, AR1_curv$c, AR1_level$phi, AR1_slope$phi, AR1_curv$phi, AR1_level$sigma2, AR1_slope$sigma2,
            AR1_curv$sigma2, rep(sigOLS2, length(tenors)))

real_result<- KF_NS(parameters, BF_5year, mats_matrix_real, 0.1)
head(real_result$y_hat) # kalman filter prediction for yield data


```
```{r}
BF_5year
```

```{r}
filtered_x <- real_result$xt_filter
predicted_x <- real_result$xt_prediction[-301,]


Beta_res <- filtered_x - predicted_x
Beta_res
```






















